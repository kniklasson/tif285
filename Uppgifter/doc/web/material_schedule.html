<div class="main_box">
    <h3>Course literature and extra resources</h3>
    <p>&nbsp;</p>
    <ol>
        <li>Lecture notes including demonstrations:
            <ul>
                <li>Christian Forss&eacute;n, <a href="https://cforssen.gitlab.io/tif285-book">Learning from data</a>. Published under a <a href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons BY-NC</a> license (2021).</li>
                <li>The book format is powered by <a href="https://jupyterbook.org/">Jupyter Book</a>.</li>
                <li>The static html book version is not interactive. However, the source files are available in the git repository. It is recommended to download all the course material using git clone (and to use git pull for possible updates). Alternatively, you can download single notebooks via the jupyter book by clicking on the download button in the upper right corner.</li>
            </ul>
        </li>
        <li>Additional reading:
            <ul>
                <li>Phil Gregory, <a href="https://doi-org.proxy.lib.chalmers.se/10.1017/CBO9780511791277"> <i>"Bayesian Logical Data Analysis for the Physical Sciences"</i></a>, Cambridge University Press (2005).</li>
                <li>David J.C. MacKay, <a href="http://www.inference.org.uk/mackay/itila/"> <i>"Information Theory, Inference, and Learning Algorithms"</i></a>, Cambridge University Press (2005).</li>
                <li>D.S. Sivia, <a href="http://search.ebscohost.com/login.aspx?direct=true&amp;db=cat07472a&amp;AN=clec.EBC430582&amp;site=eds-live&amp;scope=site"> <i>"Data Analysis : A Bayesian Tutorial"</i></a>, Oxford University Press (2006).</li>
                <li>Trevor Hastie, Robert Tibshirani, Jerome H. Friedman, <a href="http://search.ebscohost.com/login.aspx?direct=true&amp;db=cat07472a&amp;AN=clec.SPRINGERLINK9780387848587&amp;site=eds-live&amp;scope=site"> <i>"The Elements of Statistical Learning"</i></a>, Springer (2009).</li>
                <li>Andrew Gelman et al., <a href="http://search.ebscohost.com/login.aspx?direct=true&amp;db=cat06296a&amp;AN=clc.b1573576&amp;site=eds-live&amp;scope=site"> <i>"Bayesian Data Analysis"</i></a>, CRC Press, 3rd edition (2014). e-book not available.</li>
                <li>Aurelien Geron, <a href="http://search.ebscohost.com/login.aspx?direct=true&amp;db=cat07472a&amp;AN=clec.EBC4822582&amp;site=eds-live&amp;scope=site"> <i>"Hands‑On Machine Learning with Scikit‑Learn and TensorFlow"</i></a>, O'Reilly (2017).</li>
            </ul>
            Electronic versions of some of these books are available via the author or via Chalmers library (login required). See links above.
        </li>
    </ol>
    <p>&nbsp;</p>
</div>
<div class="main_box">
    <h3>Weekly schedule</h3>
    <p>Week-by-week themes and important dates:</p>
    <ul>
        <li><a href="#lv1" target="_self">Week 1 - Introduction, linear and nonlinear models, model optimization</a></li>
        <li><a href="#lv2" target="_self">Week 2 - Probability theory and Bayesian statistics, model discrepancy</a>
            <ul>
                <li>Deadline: Problem set 1 (Friday)</li>
            </ul>
        </li>
        <li><a href="#lv3" target="_self">Week 3 - Bayesian workflow</a>
            <ul>
                <li>Face-to-face discussions: Problem set 1 (Monday 15-17)</li>
            </ul>
        </li>
        <li><a href="#lv4" target="_self">Week 4 - Markov Chain Monte Carlo</a>
            <ul>
                <li>Lecture: Ethics and data analysis (Tuesday)</li>
                <li>Deadline: Problem set 2 (Friday)</li>
            </ul>
        </li>
        <li><a href="#lv5" target="_self">Week 5 - Assigning probabilities; Model selection</a>
            <ul>
                <li>Face-to-face discussions: Problem set 2 (Monday 15-17)</li>
                <li>Seminar: Ethics and data analysis (Tuesday); Compulsory for Chalmers TIF285</li>
            </ul>
        </li>
        <ul>
            <li>Deadline: Project 1 (Friday)</li>
        </ul>
        <li><a href="#lv6" target="_self">Week 6 - Gaussian processes</a></li>
        <li><a href="#lv7" target="_self">Week 7 - Neural networks</a>
            <ul>
                <li>Deadline: Problem set 3 (Friday)</li>
            </ul>
        </li>
        <li><a href="#lv8" target="_self">Week 8 - Bayesian neural networks</a>
            <ul>
                <li>Face-to-face discussions: Problem set 3 (Monday 15-17)</li>
            </ul>
        </li>
        <li>Exam week
            <ul>
                <li>Deadline: Project 2 (Wednesday)</li>
            </ul>
        </li>
    </ul>
</div>
<div class="main_box">
  The detailed course planning will be published later.
  <!--
    The course planning is preliminary. Some topics might be shifted around.
    <p>&nbsp;</p>
    <hr />
    <h4><a name="lv1"></a>Week 1 - Introduction, linear and nonlinear models, model optimization</h4>
    <ul>
        <li>Introduction (lecture notes)</li>
        <li>Getting started (demonstration)</li>
        <li>Linear regression (lecture notes)</li>
        <li>Introduction to problem set 1</li>
        <li>Extra reading: Geron ch. 4; Hastie ch. 3</li>
    </ul>
    <h4>Computer lab</h4>
    <ul>
        <li>Demonstration Monday (Christian): Getting started.</li>
        <li>instructions.</li>
        <li>Demonstration Thursday (Oliver): Introduction to problem set 1.</li>
        <li>Exercise: Jupyter/Python/NumPy intro</li>
        <li>Exercise: Linear Regression</li>
        <li>Problem set 1</li>
    </ul>
</div>
<div class="main_box">
    <hr />
    <h4><a name="lv2"></a>Week 2 - Probability theory and Bayesian statistics, model discrepancy</h4>
    <ul>
        <li>Linear regression (lecture notes)</li>
        <li>Model validation (lecture notes)</li>
        <li>Probability theory and Bayesian statistics (Gregory chs 1,2)</li>
        <li>Extra reading: Geron ch. 4; Hastie ch. 3</li>
        <li>Extra reading: Sivia ch. 1</li>
    </ul>
    <h4>Computer lab</h4>
    <ul>
        <li>Demonstration (Taylor): Introduction to project 1.</li>
        <li>Exercise: Linear Regression</li>
        <li>Exercise: Probability sum and product rules</li>
        <li>Problem set 1</li>
        <li>Project 1</li>
    </ul>
</div>
<div class="main_box">
    <hr />
    <h4><a name="lv3"></a>Week 3 - Bayesian workflow</h4>
    <ul>
        <li>Parameter estimation (lecture notes; Gregory ch 3)</li>
        <li>Extra reading: Sivia chs. 2,3</li>
        <li>Extra reading: MacKay ch. 3</li>
    </ul>
    <h4>Computer lab</h4>
    <ul>
        <li>Demonstration (Esm&eacute;e): Introduction to problem set 2.</li>
        <li>Exercise: Parameter Estimation</li>
        <li>Problem set 2</li>
        <li>Project 1</li>
    </ul>
</div>
<div class="main_box">
    <hr />
    <h4><a name="lv4"></a>Week 4 - Markov Chain Monte Carlo</h4>
    <ul>
        <li>MCMC (lecture notes; Gregory ch 12)</li>
        <li>Error propagation and nuisance parameters (lecture notes; Gregory chs 3, 9)</li>
        <li>Extra reading: Sivia chs. 3, 4</li>
        <li>Extra reading: MacKay ch. 29</li>
    </ul>
    <h4>Computer lab</h4>
    <ul>
        <li>Demonstration (Christian): Feedback on problem set 1.</li>
        <li>Exercise: MCMC</li>
        <li>Problem set 2</li>
        <li>Project 1</li>
    </ul>
</div>
<div class="main_box">
    <hr />
    <h4><a name="lv5"></a>Week 5 - Assigning probabilities; model selection</h4>
    <ul>
        <li>Assigning probabilities (lecture notes; Gregory chs 4, 8)</li>
        <li>Hypothesis testing and Bayesian model selection (lecture notes; Gregory chs 3, 7)</li>
        <li>Extra reading: Sivia chs. 4, 5</li>
        <li>Extra reading: MacKay chs. 3, 28</li>
    </ul>
    <h4>Computer lab</h4>
    <ul>
        <li>Demonstration (Esm&eacute;e): Feedback on problem set 2.</li>
        <li>Exercise: Assigning probabilities (follow the lecture demonstration)</li>
        <li>Project 1</li>
    </ul>
</div>
<div class="main_box">
    <hr />
    <h4><a name="lv6"></a>Week 6 - Gaussian processes</h4>
    <ul>
        <li>Gaussian processes, part 1 (lecture notes; MacKay ch 45)</li>
        <li>Gaussian processes, part 2 (lecture notes; MacKay ch 45)</li>
        <li>Extra reading:</li>
    </ul>
    <h4>Computer lab</h4>
    <ul>
        <li>Demonstration (Taylor): Feedback on project 1.</li>
        <li>Demonstration (Oliver): Introduction to problem set 3.</li>
        <li>Exercise: Gaussian processes</li>
        <li>Problem set 3</li>
    </ul>
</div>
<div class="main_box">
    <hr />
    <h4><a name="lv7"></a>Week 7 -Neural networks</h4>
    <ul>
        <li>Classification, logistic regression (lecture notes; MacKay chs 38,39)</li>
        <li>Neural networks (lecture notes; MacKay chs 38,39)</li>
        <li>Extra reading:</li>
    </ul>
    <h4>Computer lab</h4>
    <ul>
        <li>Demonstration (Anna): Introduction to project 2.</li>
        <li>Exercise: Neural networks</li>
        <li>Problem set 3</li>
        <li>Project 2</li>
    </ul>
</div>
<div class="main_box">
    <hr />
    <h4><a name="lv8"></a>Week 8 - Bayesian neural networks</h4>
    <ul>
        <li>Bayesian neural networks (lecture notes; MacKay ch 41)</li>
        <li>Deep neural networks (lecture notes; MacKay ch 44)</li>
    </ul>
    <h4>Computer lab</h4>
    <ul>
        <li>Demonstration: TBD.</li>
        <li>Project 2</li>
      </ul>
      -->
</div>